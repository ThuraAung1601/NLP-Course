{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Processing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text preprocessing using NLTK Library"
      ],
      "metadata": {
        "id": "bu4H9AkiwxkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "i4GHNGauw7yN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgRvKUxuw2Ju",
        "outputId": "e55b662a-49d2-4ba9-fb48-3c19511ef5e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Token the input sentence into words and punctuation marks. Even if the punctuation is stick to previous and next word."
      ],
      "metadata": {
        "id": "80_XUkaxxKp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized with white spaces('\\t\\n\\s') and puncuation marks\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sentence = \"The price\\t of buger \\nin BurgerKing is 12$.\\n\"\n",
        "print(word_tokenize(sentence))\n",
        "sentence = \"This is an example!An example of tokenization.\\n\"\n",
        "print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHIsgZgNw-No",
        "outputId": "c88b53c4-b76b-442c-fb5a-5ea07df76286"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'price', 'of', 'buger', 'in', 'BurgerKing', 'is', '12', '$', '.']\n",
            "['This', 'is', 'an', 'example', '!', 'An', 'example', 'of', 'tokenization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized only with white spaces('\\t\\n\\s')\n",
        "\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "tokenizer = WhitespaceTokenizer()\n",
        "sentence = \"The price\\t of buger \\nin BurgerKing is 12$.\\n\"\n",
        "print(tokenizer.tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45qonvIfx2W3",
        "outputId": "cb376c8c-5343-45a1-ad59-f75061a43711"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'price', 'of', 'buger', 'in', 'BurgerKing', 'is', '12$.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized only with regex pattern we define\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# words with numbers and characters are only token and others are removed\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "print(tokenizer.tokenize(sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX7db_L3yysA",
        "outputId": "a24a3b68-07a7-4b90-a122-fc9db019d094"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'price', 'of', 'buger', 'in', 'BurgerKing', 'is', '12']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized with alphabetic and non alphabetic(all punctutaion one token)\n",
        "\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "tokenizer = WordPunctTokenizer()\n",
        "print(tokenizer.tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7KqOigOziph",
        "outputId": "736ead02-e92f-44dd-abae-ce11942bf349"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'price', 'of', 'buger', 'in', 'BurgerKing', 'is', '12', '$.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized based on sentence (used when we have paragraph)\n",
        "# split using . operator \n",
        "# only split if after . we have space \n",
        "# if the are sticked together or we have space before . we take them one sentence\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "paragraph = \"Hello everyone . This is fourth session. Thank you for attending.\"\n",
        "print(sent_tokenize(paragraph))\n",
        "paragraph = \"Hello everyone . This is fourth session.Thank you for attending.\"\n",
        "print(sent_tokenize(paragraph))\n",
        "paragraph = \"Hello everyone . This is fourth session .Thank you for attending.\"\n",
        "print(sent_tokenize(paragraph))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBqg4RkN0RZK",
        "outputId": "8227b1a8-8d9d-4edb-f670-7a6ff7fadc9f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello everyone .', 'This is fourth session.', 'Thank you for attending.']\n",
            "['Hello everyone .', 'This is fourth session.Thank you for attending.']\n",
            "['Hello everyone .', 'This is fourth session .Thank you for attending.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization"
      ],
      "metadata": {
        "id": "i3QiZ53K0MFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It depends on the task we are doing."
      ],
      "metadata": {
        "id": "GvPBHeTv1_bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# normalized by lowercasing all words\n",
        "tokens = word_tokenize(sentence.lower())\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reL7OeHB0y-4",
        "outputId": "dc9c9ad3-1325-43dc-c6dc-20b7b82e1032"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'price', 'of', 'buger', 'in', 'burgerking', 'is', '12', '$', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the non-alphabetic words\n",
        "tokens = word_tokenize(sentence)\n",
        "tokens = [w for w in tokens if w.isalpha()]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id5d5wR52ZI4",
        "outputId": "38e3bb70-33a2-41ae-da8f-5218f3d6fccd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'price', 'of', 'buger', 'in', 'BurgerKing', 'is']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "GXRD5tWx28MV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOsOh_Wp3CMr",
        "outputId": "f32a3976-94e1-4c78-c3a2-82ddf991388f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"been had done languages cities mice\"\n",
        "\n",
        "# first we tokenize the word of sentences\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "tokens_v = []\n",
        "tokens_n = []\n",
        "# you should define pos tag (v = verb, n=noun)\n",
        "for word in words:\n",
        "  # verbs has been converted to default\n",
        "  tokens_v.append(lemmatizer.lemmatize(word,pos='v'))\n",
        "  # nouns has been converted to default\n",
        "  tokens_n.append(lemmatizer.lemmatize(word,pos='n'))\n",
        "print(tokens_v)\n",
        "print(tokens_n)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPEGQUkN4vG-",
        "outputId": "2cfbbd77-0c8f-46a6-a86d-c33074878bcb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['be', 'have', 'do', 'languages', 'cities', 'mice']\n",
            "['been', 'had', 'done', 'language', 'city', 'mouse']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can define pos tag of a each word based on the lemma it has."
      ],
      "metadata": {
        "id": "eHcbey1M458-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "IlnkIib45MaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# snowball stemmer support different languages and we should define the language in constructor\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "p_stemmer = PorterStemmer()\n",
        "s_stemmer = SnowballStemmer('english')\n",
        "\n",
        "sentence = \"There are several types of stemming algorithms.\"\n",
        "# first we need to tokenize the sentence\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "tokens_p = []\n",
        "tokens_s = []\n",
        "\n",
        "for word in words:\n",
        "  tokens_p.append(p_stemmer.stem(word))\n",
        "  tokens_s.append(s_stemmer.stem(word))\n",
        "\n",
        "print(tokens_p)\n",
        "print(tokens_s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjX465uv5PJe",
        "outputId": "e1f6b392-5385-4141-9553-e178a18820d4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['there', 'are', 'sever', 'type', 'of', 'stem', 'algorithm', '.']\n",
            "['there', 'are', 'sever', 'type', 'of', 'stem', 'algorithm', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopword"
      ],
      "metadata": {
        "id": "Eah9Zd4I6rfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjIIIp3o7ASC",
        "outputId": "5da64e12-2b86-4af7-d98f-8a5eddaf88f6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "sentence = \"There are several types of stemming algorithms.\"\n",
        "\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "token_1 = []\n",
        "token_2 = []\n",
        "\n",
        "for word in words:\n",
        "  if word.lower() not in stop_words:\n",
        "    token_1.append(word.lower())\n",
        "print(token_1)\n",
        "\n",
        "# add a stopword \n",
        "stop_words.extend(['types'])\n",
        "\n",
        "for word in words:\n",
        "  if word.lower() not in stop_words:\n",
        "    token_2.append(word.lower())\n",
        "print(token_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjhShNRl64B3",
        "outputId": "8967bea7-7605-478d-d0ab-c050ead7e938"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['several', 'types', 'stemming', 'algorithms', '.']\n",
            "['several', 'stemming', 'algorithms', '.']\n"
          ]
        }
      ]
    }
  ]
}