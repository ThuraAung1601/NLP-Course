{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MCI-NLP-HW4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffnR6-SpR5Oh"
      },
      "source": [
        "!pip install hazm\n",
        "!pip install parsivar\n",
        "!pip install persianutils\n",
        "!pip install polyglot\n",
        "!pip install pyicu\n",
        "!pip install pycld2\n",
        "!pip install morfessor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ7UxZvpSXnN"
      },
      "source": [
        "import hazm\n",
        "import parsivar\n",
        "import persianutils\n",
        "import polyglot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHMcyuBqRE6T",
        "outputId": "55d82eda-f2be-45d3-ee3f-b0aecd0c377b"
      },
      "source": [
        "# train.csv\n",
        "!gdown --id 1rovazK48q7pHcEM271aX70Dr594NYQ77\n",
        "# test.csv\n",
        "!gdown --id 1ZCHuj6JtyOkb5ismn3qF3Rp2DRGJ1tRk\n",
        "\n",
        "# Q1-Sentences.txt\n",
        "!gdown --id 1ARhYtjRLG729JEzkxMxlKjRKb6mwhEfc\n",
        "# Q6-Sentences.txt\n",
        "!gdown --id 1QLR5ax5KZGDH7bkQf1c9Hp59BILsiI3Q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rovazK48q7pHcEM271aX70Dr594NYQ77\n",
            "To: /content/train.csv\n",
            "20.1MB [00:00, 48.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZCHuj6JtyOkb5ismn3qF3Rp2DRGJ1tRk\n",
            "To: /content/test.csv\n",
            "100% 2.03M/2.03M [00:00<00:00, 64.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ARhYtjRLG729JEzkxMxlKjRKb6mwhEfc\n",
            "To: /content/Q1-Sentences.txt\n",
            "100% 734/734 [00:00<00:00, 639kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QLR5ax5KZGDH7bkQf1c9Hp59BILsiI3Q\n",
            "To: /content/Q6-Sentences.txt\n",
            "100% 500/500 [00:00<00:00, 800kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irnQVPujZAK0"
      },
      "source": [
        "# **Q1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXTAPJYAY_Yk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d0c0298-65fb-461f-f773-efd28414f7f8"
      },
      "source": [
        "# sentences = [\"انتخابات هفته آینده رادر تهران برگزار میکنیم ، اما دسته بندی حوزهها برای رأی گیری از این هفته آغاز شده است\",\n",
        "#              \"توانتشارخبرمهم در رسانه ها را تایید کردی\",\n",
        "#              \"پسردکتراحمدی درکارنامه حرفه ای خود ۱۸سریال تلویزیونی و ۱۲فیلم سینمایی دارد\",\n",
        "#              \"نقّاشی ساختمان به پایان رسیدددددددد\",\n",
        "#              \"اللة اکبر گویان وارد مسجد شد\",\n",
        "#              \"دکتر جان مممممممممممممممنونم که مشکلات ﺣﺮﻛﺘﻲ فرزندم را درمان کردید\",\n",
        "#              \"من قبل از علی رسیدم\",\n",
        "#              \"من بعد از علی رسیدم\",\n",
        "#              \"من بعد از ظهر رسیدم\",\n",
        "#              \"من قبل از ظهر رسیدم\"]\n",
        "\n",
        "sentences = []\n",
        "\n",
        "with open(\"Q1-Sentences.txt\") as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "for line in lines:\n",
        "  sentences.append(line.split(':')[1].strip())\n",
        "\n",
        "sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['انتخابات هفته آینده رادر تهران برگزار میکنیم ، اما دسته بندی حوزهها برای رأی گیری از این هفته آغاز شده است',\n",
              " 'توانتشارخبرمهم در رسانه ها را تایید کردی',\n",
              " 'پسردکتراحمدی درکارنامه حرفه ای خود ۱۸سریال تلویزیونی و ۱۲فیلم سینمایی دارد',\n",
              " 'نقّاشی ساختمان به پایان رسیدددددددد',\n",
              " 'اللة اکبر گویان وارد مسجد شد',\n",
              " 'دکتر جان مممممممممممممممنونم که مشکلات حرکتي فرزندم را درمان کردید']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg4XwWa5RoxE"
      },
      "source": [
        "# **Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z6PSFVDXrA8"
      },
      "source": [
        "# **Hazm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hewcE1eRKZxp",
        "outputId": "43d3d802-effc-4378-84b0-571856a86f61"
      },
      "source": [
        "for sent in sentences:\n",
        "  print(hazm.word_tokenize(sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['انتخابات', 'هفته', 'آینده', 'رادر', 'تهران', 'برگزار', 'میکنیم', '،', 'اما', 'دسته', 'بندی', 'حوزهها', 'برای', 'رأی', 'گیری', 'از', 'این', 'هفته', 'آغاز', 'شده_است']\n",
            "['توانتشارخبرمهم', 'در', 'رسانه', 'ها', 'را', 'تایید', 'کردی']\n",
            "['پسردکتراحمدی', 'درکارنامه', 'حرفه', 'ای', 'خود', '۱۸', 'سریال', 'تلویزیونی', 'و', '۱۲', 'فیلم', 'سینمایی', 'دارد']\n",
            "['نقّاشی', 'ساختمان', 'به', 'پایان', 'رسیدددددددد']\n",
            "['اللة', 'اکبر', 'گویان', 'وارد', 'مسجد', 'شد']\n",
            "['دکتر', 'جان', 'مممممممممممممممنونم', 'که', 'مشکلات', 'حرکتي', 'فرزندم', 'را', 'درمان', 'کردید']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA-OIvNsXv28"
      },
      "source": [
        "# **Parsivar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JL7WjQFdYHHR",
        "outputId": "371fd468-65bd-4c92-dcce-39ccd14553c5"
      },
      "source": [
        "from parsivar import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "for sent in sentences:\n",
        "  print(tokenizer.tokenize_words(sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['انتخابات', 'هفته', 'آینده', 'رادر', 'تهران', 'برگزار', 'میکنیم', '،', 'اما', 'دسته', 'بندی', 'حوزهها', 'برای', 'رأی', 'گیری', 'از', 'این', 'هفته', 'آغاز', 'شده', 'است']\n",
            "['توانتشارخبرمهم', 'در', 'رسانه', 'ها', 'را', 'تایید', 'کردی']\n",
            "['پسردکتراحمدی', 'درکارنامه', 'حرفه', 'ای', 'خود', '۱۸سریال', 'تلویزیونی', 'و', '۱۲فیلم', 'سینمایی', 'دارد']\n",
            "['نقّاشی', 'ساختمان', 'به', 'پایان', 'رسیدددددددد']\n",
            "['اللة', 'اکبر', 'گویان', 'وارد', 'مسجد', 'شد']\n",
            "['دکتر', 'جان', 'مممممممممممممممنونم', 'که', 'مشکلات', 'حرکتي', 'فرزندم', 'را', 'درمان', 'کردید']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKOAzj4zhfCi"
      },
      "source": [
        "## **Polyglot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyhnQxfnhlyM",
        "outputId": "57dd0222-0031-4b67-b0f1-57758af9f9f6"
      },
      "source": [
        "from polyglot.text import Text\n",
        "\n",
        "for sent in sentences:\n",
        "  print(Text(sent).words)\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['انتخابات', 'هفته', 'آینده', 'رادر', 'تهران', 'برگزار', 'میکنیم', '،', 'اما', 'دسته', 'بندی', 'حوزهها', 'برای', 'رأی', 'گیری', 'از', 'این', 'هفته', 'آغاز', 'شده', 'است']\n",
            "['توانتشارخبرمهم', 'در', 'رسانه', 'ها', 'را', 'تایید', 'کردی']\n",
            "['پسردکتراحمدی', 'درکارنامه', 'حرفه', 'ای', 'خود', '۱۸سریال', 'تلویزیونی', 'و', '۱۲فیلم', 'سینمایی', 'دارد']\n",
            "['نقّاشی', 'ساختمان', 'به', 'پایان', 'رسیدددددددد']\n",
            "['اللة', 'اکبر', 'گویان', 'وارد', 'مسجد', 'شد']\n",
            "['دکتر', 'جان', 'مممممممممممممممنونم', 'که', 'مشکلات', 'حرکتي', 'فرزندم', 'را', 'درمان', 'کردید']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGyL3WHHY1K-"
      },
      "source": [
        "# **Normalization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XT7z7BXZN_K"
      },
      "source": [
        "# **Hazm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc2jk1kHZRRq",
        "outputId": "0d2a3d83-dcc3-49fb-8a63-2ca11458004f"
      },
      "source": [
        "from hazm import Normalizer\n",
        "\n",
        "normalizer = Normalizer()\n",
        "\n",
        "\n",
        "for sent in sentences:\n",
        "  print(normalizer.normalize(sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "انتخابات هفته آینده رادر تهران برگزار میکنیم، اما دسته بندی حوزهها برای رأی گیری از این هفته آغاز شده است\n",
            "توانتشارخبرمهم در رسانه‌ها را تایید کردی\n",
            "پسردکتراحمدی درکارنامه حرفه‌ای خود ۱۸سریال تلویزیونی و ۱۲فیلم سینمایی دارد\n",
            "نقاشی ساختمان به پایان رسیدددددددد\n",
            "اللة اکبر گویان وارد مسجد شد\n",
            "دکتر جان مممممممممممممممنونم که مشکلات حرکتی فرزندم را درمان کردید\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mji_X0opYD7h",
        "outputId": "fadd498b-b19a-4dfd-b8f4-f3ad90f7b0d3"
      },
      "source": [
        "from hazm import InformalNormalizer\n",
        "\n",
        "normalizer = InformalNormalizer()\n",
        "\n",
        "\n",
        "for sent in sentences:\n",
        "  print(normalizer.normalize(sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[['انتخابات'], ['هفته'], ['آینده'], ['رادر'], ['تهران'], ['برگزار'], ['می\\u200cکنیم', 'میکنیم'], ['،'], ['اما'], ['دسته'], ['بندی'], ['حوزهها'], ['برای'], ['رأی'], ['گیری'], ['از'], ['این'], ['هفته'], ['آغاز'], ['شده_است']]]\n",
            "[[['توانتشارخبرمهم'], ['در'], ['رسانه\\u200cها'], ['را'], ['تایید'], ['کردی']]]\n",
            "[[['پسردکتراحمدی'], ['درکارنامه'], ['حرفه\\u200cای'], ['خود'], ['۱۸'], ['سریال'], ['تلویزیونی'], ['و'], ['۱۲'], ['فیلم'], ['سینمایی'], ['دارد']]]\n",
            "[[['نقاشی'], ['ساختمان'], ['به'], ['پایان'], ['رسیدددددددد']]]\n",
            "[[['اللة'], ['اکبر'], ['گویان'], ['وارد'], ['مسجد'], ['شد']]]\n",
            "[[['دکتر'], ['جان'], ['مممممممممممممممنونم'], ['که'], ['مشکلات'], ['حرکتی'], ['فرزندم'], ['را'], ['درمان'], ['کردید']]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6_oGO5wYaec",
        "outputId": "09ecff29-4853-4738-ec11-cd8d4c6ceda3"
      },
      "source": [
        "from hazm import Normalizer, InformalNormalizer\n",
        "\n",
        "normalizer = Normalizer(persian_style=True, persian_numbers=True, remove_diacritics=True, token_based=True, affix_spacing=True)\n",
        "informal_normalizer = InformalNormalizer()\n",
        "\n",
        "for sent in sentences:\n",
        "  print(sent)\n",
        "  norm_sent = informal_normalizer.normalize(sent)\n",
        "  norm_sent = [word[0] for word in norm_sent[0]]\n",
        "  norm_sent = ' '.join(norm_sent)\n",
        "  norm_sent = normalizer.normalize(norm_sent)\n",
        "  print(norm_sent)\n",
        "  print('***********************************')\n",
        "\t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "انتخابات هفته آینده رادر تهران برگزار میکنیم ، اما دسته بندی حوزهها برای رأی گیری از این هفته آغاز شده است\n",
            "انتخابات هفته آینده رادر تهران برگزار می‌کنیم، اما دسته‌بندی حوزهها برای رأی‌گیری از این هفته آغاز شده_است\n",
            "***********************************\n",
            "توانتشارخبرمهم در رسانه ها را تایید کردی\n",
            "توانتشارخبرمهم در رسانه‌ها را تایید کردی\n",
            "***********************************\n",
            "پسردکتراحمدی درکارنامه حرفه ای خود ۱۸سریال تلویزیونی و ۱۲فیلم سینمایی دارد\n",
            "پسردکتراحمدی درکارنامه حرفه‌ای خود ۱۸ سریال تلویزیونی و ۱۲ فیلم سینمایی دارد\n",
            "***********************************\n",
            "نقّاشی ساختمان به پایان رسیدددددددد\n",
            "نقاشی ساختمان به پایان رسیدددددددد\n",
            "***********************************\n",
            "اللة اکبر گویان وارد مسجد شد\n",
            "اللة اکبر گویان وارد مسجد شد\n",
            "***********************************\n",
            "دکتر جان مممممممممممممممنونم که مشکلات حرکتي فرزندم را درمان کردید\n",
            "دکتر جان مممممممممممممممنونم که مشکلات حرکتی فرزندم را درمان کردید\n",
            "***********************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcFhtRVzbrtT",
        "outputId": "ae726d94-98da-4f72-938e-4ed5617ddbef"
      },
      "source": [
        "from hazm import Normalizer\n",
        "\n",
        "normalizer = Normalizer(persian_style=True, persian_numbers=True, remove_diacritics=True, token_based=True, affix_spacing=True)\n",
        "\n",
        "\n",
        "for sent in sentences:\n",
        "  print(sent)\n",
        "  print(normalizer.normalize(sent))\n",
        "  print(\"******************************\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "انتخابات هفته آینده رادر تهران برگزار میکنیم ، اما دسته بندی حوزهها برای رأی گیری از این هفته آغاز شده است\n",
            "انتخابات هفته آینده رادر تهران برگزار میکنیم، اما دسته‌بندی حوزهها برای رأی‌گیری از این هفته آغاز شده است\n",
            "******************************\n",
            "توانتشارخبرمهم در رسانه ها را تایید کردی\n",
            "توانتشارخبرمهم در رسانه‌ها را تایید کردی\n",
            "******************************\n",
            "پسردکتراحمدی درکارنامه حرفه ای خود ۱۸سریال تلویزیونی و ۱۲فیلم سینمایی دارد\n",
            "پسردکتراحمدی درکارنامه حرفه‌ای خود ۱۸ سریال تلویزیونی و ۱۲ فیلم سینمایی دارد\n",
            "******************************\n",
            "نقّاشی ساختمان به پایان رسیدددددددد\n",
            "نقاشی ساختمان به پایان رسیدددددددد\n",
            "******************************\n",
            "اللة اکبر گویان وارد مسجد شد\n",
            "اللة اکبر گویان وارد مسجد شد\n",
            "******************************\n",
            "دکتر جان مممممممممممممممنونم که مشکلات حرکتي فرزندم را درمان کردید\n",
            "دکتر جان مممممممممممممممنونم که مشکلات حرکتی فرزندم را درمان کردید\n",
            "******************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRpSJapaZmE2"
      },
      "source": [
        "# **Parsivar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMkA8aw9Zlvd",
        "outputId": "b262a550-1bab-428b-946c-74dd5b99a92b"
      },
      "source": [
        "from parsivar import Normalizer\n",
        "\n",
        "normalizer = Normalizer()\n",
        "\n",
        "for sent in sentences:\n",
        "  print(normalizer.normalize(sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "انتخابات هفته آینده رادر تهران برگزار می‌کنیم ، اما دسته‌بندی حوزهها برای رای‌گیری از این هفته آغاز‌شده‌است\n",
            "توانتشارخبرمهم در رسانه‌ها را تایید کردی\n",
            "پسردکتراحمدی درکارنامه حرفه‌ای خود 18سریال تلویزیونی و 12فیلم سینمایی دارد\n",
            "نقّاشی ساختمان به پایان رسیدددددددد\n",
            "الله اکبر گویان وارد مسجد شد\n",
            "دکتر جان مممممممممممممممنونم که مشکلات حرکتی فرزندم را درمان کردید\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC0WZutrhEXa"
      },
      "source": [
        "# **Persianutils**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q22UzcEhPZK",
        "outputId": "15ecbea9-00bf-40a5-c0e8-9ca6ef99fc91"
      },
      "source": [
        "import persianutils as pu\n",
        "\n",
        "for sent in sentences:\n",
        "  print(pu.standardize(sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "انتخابات هفته اینده رادر تهران برگزار میکنیم ، اما دسته بندی حوزهها برای رای گیری از این هفته اغاز شده است\n",
            "توانتشارخبرمهم در رسانه ها را تایید کردی\n",
            "پسردکتراحمدی درکارنامه حرفه ای خود ۱۸سریال تلویزیونی و ۱۲فیلم سینمایی دارد\n",
            "نقاشی ساختمان به پایان رسیدددددددد\n",
            "الله اکبر گویان وارد مسجد شد\n",
            "دکتر جان مممممممممممممممنونم که مشکلات حرکتی فرزندم را درمان کردید\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlfMQ9Q7W_Bu"
      },
      "source": [
        "# **Q2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fUyuWKOaNzM"
      },
      "source": [
        "# **Stemming**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM5Q-K7jaTFu"
      },
      "source": [
        "# **Hazm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBp1u1jeeYeM",
        "outputId": "3c15296d-299e-42b2-dcd5-ca3d26f83f50"
      },
      "source": [
        "from hazm import Stemmer\n",
        "\n",
        "stemmer = Stemmer()\n",
        "\n",
        "for sent in sentences:\n",
        "  stems = []\n",
        "  print(sent)\n",
        "  words = sent.split(\" \")\n",
        "  for word in words:\n",
        "    stems.append(stemmer.stem(word))\n",
        "  print(stems)\n",
        "  print(\"**********************\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "انتخابات هفته آینده رادر تهران برگزار میکنیم ، اما دسته بندی حوزهها برای رأی گیری از این هفته آغاز شده است\n",
            "['انتخاب', 'هفته', 'آینده', 'رادر', 'تهر', 'برگزار', 'میکن', '،', 'اما', 'دسته', 'بند', 'حوزه', 'برا', 'رأ', 'گیر', 'از', 'این', 'هفته', 'آغاز', 'شده', 'اس']\n",
            "**********************\n",
            "توانتشارخبرمهم در رسانه ها را تایید کردی\n",
            "['توانتشارخبرمه', 'در', 'رسانه', '', 'را', 'تایید', 'کرد']\n",
            "**********************\n",
            "پسردکتراحمدی درکارنامه حرفه ای خود ۱۸سریال تلویزیونی و ۱۲فیلم سینمایی دارد\n",
            "['پسردکتراحمد', 'درکارنامه', 'حرفه', 'ا', 'خود', '۱۸سریال', 'تلویزیون', 'و', '۱۲فیل', 'سینما', 'دارد']\n",
            "**********************\n",
            "نقّاشی ساختمان به پایان رسیدددددددد\n",
            "['نقّاش', 'ساخ', 'به', 'پا', 'رسیدددددددد']\n",
            "**********************\n",
            "اللة اکبر گویان وارد مسجد شد\n",
            "['اللة', 'اکبر', 'گو', 'وارد', 'مسجد', 'شد']\n",
            "**********************\n",
            "دکتر جان مممممممممممممممنونم که مشکلات حرکتي فرزندم را درمان کردید\n",
            "['دک', 'ج', 'مممممممممممممممنون', 'که', 'مشکل', 'حرکتي', 'فرزند', 'را', 'در', 'کردید']\n",
            "**********************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eKhRIOoaWdq"
      },
      "source": [
        "# **Parsivar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7cB2ZALfmro",
        "outputId": "5890b957-c995-4c97-96c0-f39e81804760"
      },
      "source": [
        "from parsivar import FindStems\n",
        "from parsivar import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "stemmer = FindStems()\n",
        "\n",
        "for sent in sentences:\n",
        "  stems = []\n",
        "  print(sent)\n",
        "  words = sent.split(\" \")\n",
        "  for word in words:\n",
        "    stems.append(stemmer.convert_to_stem(word))\n",
        "  print(stems)\n",
        "  print(\"**********************\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "انتخابات هفته آینده رادر تهران برگزار میکنیم ، اما دسته بندی حوزهها برای رأی گیری از این هفته آغاز شده است\n",
            "['انتخابات', 'هفته', 'آینده', 'رادر', 'تهران', 'برگزار', 'کرد&کن', '،', 'اما', 'دسته', 'بند', 'حوزه', 'برای', 'رأی', 'گرفت&گیر', 'از', 'این', 'هفته', 'آغاز', 'شده', 'اس']\n",
            "**********************\n",
            "توانتشارخبرمهم در رسانه ها را تایید کردی\n",
            "['توانتشارخبرمهم', 'در', 'رسانه', 'ها', 'را', 'تایید', 'کردی']\n",
            "**********************\n",
            "پسردکتراحمدی درکارنامه حرفه ای خود ۱۸سریال تلویزیونی و ۱۲فیلم سینمایی دارد\n",
            "['پسردکتراحمدی', 'درکارنامه', 'حرفه', 'ای', 'خود', '۱۸سریال', 'تلویزیونی', 'و', '۱۲فیلم', 'سینمایی', 'داشت&دارد']\n",
            "**********************\n",
            "نقّاشی ساختمان به پایان رسیدددددددد\n",
            "['نقّاشی', 'ساختمان', 'به', 'پایان', 'رسیدددددددد']\n",
            "**********************\n",
            "اللة اکبر گویان وارد مسجد شد\n",
            "['اللة', 'اکبر', 'گوی', 'وارد', 'مسجد', 'شد']\n",
            "**********************\n",
            "دکتر جان مممممممممممممممنونم که مشکلات حرکتي فرزندم را درمان کردید\n",
            "['دکتر', 'جان', 'مممممممممممممممنونم', 'که', 'مشکلات', 'حرکتي', 'فرزند', 'را', 'درمان', 'کرد&کن']\n",
            "**********************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VOAeuasabYa"
      },
      "source": [
        "# **Lemmatization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltrbr6Dlai2L"
      },
      "source": [
        "# **Hazm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f0vHmVbqD-f",
        "outputId": "87f654ab-4b6d-459d-d0b5-ec6e405016b1"
      },
      "source": [
        "from hazm import Lemmatizer\n",
        "\n",
        "lemmatizer = Lemmatizer()\n",
        "\n",
        "for sent in sentences:\n",
        "  lems = []\n",
        "  print(sent)\n",
        "  words = sent.split(\" \")\n",
        "  for word in words:\n",
        "    lems.append(lemmatizer.lemmatize(word))\n",
        "  print(lems)\n",
        "  print(\"**********************\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "انتخابات هفته آینده رادر تهران برگزار میکنیم ، اما دسته بندی حوزهها برای رأی گیری از این هفته آغاز شده است\n",
            "['انتخابات', 'هفته', 'آینده', 'رادر', 'تهران', 'برگزار', 'میکنیم', '،', 'اما', 'دسته', 'بست#بند', 'حوزه', 'برای', 'رأی', 'گرفت#گیر', 'از', 'این', 'هفته', 'آغاز', 'شده', '#است']\n",
            "**********************\n",
            "توانتشارخبرمهم در رسانه ها را تایید کردی\n",
            "['توانتشارخبرمهم', 'در', 'رسانه', 'ها', 'را', 'تایید', 'کردی']\n",
            "**********************\n",
            "پسردکتراحمدی درکارنامه حرفه ای خود ۱۸سریال تلویزیونی و ۱۲فیلم سینمایی دارد\n",
            "['پسردکتراحمدی', 'درکارنامه', 'حرفه', 'ای', 'خود', '۱۸سریال', 'تلویزیون', 'و', '۱۲فیلم', 'سینمایی', 'داشت#دار']\n",
            "**********************\n",
            "نقّاشی ساختمان به پایان رسیدددددددد\n",
            "['نقّاش', 'ساختمان', 'به', 'پایان', 'رسیدددددددد']\n",
            "**********************\n",
            "اللة اکبر گویان وارد مسجد شد\n",
            "['اللة', 'اکبر', 'گویان', 'وارد', 'مسجد', 'شد#شو']\n",
            "**********************\n",
            "دکتر جان مممممممممممممممنونم که مشکلات حرکتي فرزندم را درمان کردید\n",
            "['دکتر', 'جان', 'مممممممممممممممنونم', 'که', 'مشکلات', 'حرکتي', 'فرزند', 'را', 'درمان', 'کرد#کن']\n",
            "**********************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ0YTenmamMA"
      },
      "source": [
        "# **Parsivar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR1bN9YHaqX0"
      },
      "source": [
        "# Not Exist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTzrpEDDarhV"
      },
      "source": [
        "# **Stopword Removal**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFRSsf45a0ns"
      },
      "source": [
        "# **Hazm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDjherPu3B4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b07991-2fe7-4593-d34f-d0b159ae1e06"
      },
      "source": [
        "!git clone https://github.com/sobhe/hazm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hazm'...\n",
            "remote: Enumerating objects: 2282, done.\u001b[K\n",
            "remote: Counting objects: 100% (394/394), done.\u001b[K\n",
            "remote: Compressing objects: 100% (238/238), done.\u001b[K\n",
            "remote: Total 2282 (delta 222), reused 296 (delta 137), pack-reused 1888\u001b[K\n",
            "Receiving objects: 100% (2282/2282), 2.65 MiB | 19.50 MiB/s, done.\n",
            "Resolving deltas: 100% (1467/1467), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_omPG0-yYK9",
        "outputId": "0595e686-6353-43d5-97ff-f35877d66007"
      },
      "source": [
        "from hazm import stopwords_list\n",
        "\n",
        "stopwords = stopwords_list(\"/content/hazm/hazm/data/stopwords.dat\")\n",
        "\n",
        "for sent in sentences:\n",
        "  sents_without_stopword = []\n",
        "  print(sent)\n",
        "  words = sent.split(\" \")\n",
        "  for word in words:\n",
        "    if word not in stopwords:\n",
        "      sents_without_stopword.append(word)\n",
        "  print(sents_without_stopword)\n",
        "  print(\"**********************\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "انتخابات هفته آینده رادر تهران برگزار میکنیم ، اما دسته بندی حوزهها برای رأی گیری از این هفته آغاز شده است\n",
            "['انتخابات', 'هفته', 'آینده', 'رادر', 'تهران', 'برگزار', 'میکنیم', '،', 'حوزهها', 'رأی', 'هفته', 'آغاز']\n",
            "**********************\n",
            "توانتشارخبرمهم در رسانه ها را تایید کردی\n",
            "['توانتشارخبرمهم', 'رسانه', 'ها', 'تایید', 'کردی']\n",
            "**********************\n",
            "پسردکتراحمدی درکارنامه حرفه ای خود ۱۸سریال تلویزیونی و ۱۲فیلم سینمایی دارد\n",
            "['پسردکتراحمدی', 'درکارنامه', 'حرفه', 'ای', '۱۸سریال', 'تلویزیونی', '۱۲فیلم', 'سینمایی']\n",
            "**********************\n",
            "نقّاشی ساختمان به پایان رسیدددددددد\n",
            "['نقّاشی', 'ساختمان', 'پایان', 'رسیدددددددد']\n",
            "**********************\n",
            "اللة اکبر گویان وارد مسجد شد\n",
            "['اللة', 'اکبر', 'گویان', 'مسجد']\n",
            "**********************\n",
            "دکتر جان مممممممممممممممنونم که مشکلات حرکتي فرزندم را درمان کردید\n",
            "['دکتر', 'جان', 'مممممممممممممممنونم', 'مشکلات', 'حرکتي', 'فرزندم', 'درمان', 'کردید']\n",
            "**********************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSiAOO-Ba4Mr"
      },
      "source": [
        "# **Parsivar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAhWeucpa6sb"
      },
      "source": [
        "# Not Exist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP79isoP43dA"
      },
      "source": [
        "# **Q3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbLElKWe491P"
      },
      "source": [
        "# **Stemming**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJB1Ns325LcI"
      },
      "source": [
        "# **Hazm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYId92PB49BA",
        "outputId": "3bb888f7-f544-4ffc-dd74-88725e3d3264"
      },
      "source": [
        "from hazm import Normalizer\n",
        "from hazm import Stemmer\n",
        "\n",
        "normalizer = Normalizer()\n",
        "stemmer = Stemmer()\n",
        "\n",
        "for sent in sentences:\n",
        "  stems = []\n",
        "  print(sent)\n",
        "  words = hazm.word_tokenize(normalizer.normalize(sent))\n",
        "  for word in words:\n",
        "    stems.append(stemmer.stem(word))\n",
        "  print(stems)\n",
        "  print(\"**********************\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "انتخابات هفته آینده رادر تهران برگزار میکنیم ، اما دسته بندی حوزهها برای رأی گیری از این هفته آغاز شده است\n",
            "['انتخاب', 'هفته', 'آینده', 'رادر', 'تهر', 'برگزار', 'میکن', '،', 'اما', 'دسته', 'بند', 'حوزه', 'برا', 'رأ', 'گیر', 'از', 'این', 'هفته', 'آغاز', 'شده_اس']\n",
            "**********************\n",
            "توانتشارخبرمهم در رسانه ها را تایید کردی\n",
            "['توانتشارخبرمه', 'در', 'رسانه', 'را', 'تایید', 'کرد']\n",
            "**********************\n",
            "پسردکتراحمدی درکارنامه حرفه ای خود ۱۸سریال تلویزیونی و ۱۲فیلم سینمایی دارد\n",
            "['پسردکتراحمد', 'درکارنامه', 'حرفه', 'خود', '۱۸', 'سریال', 'تلویزیون', 'و', '۱۲', 'فیل', 'سینما', 'دارد']\n",
            "**********************\n",
            "نقّاشی ساختمان به پایان رسیدددددددد\n",
            "['نقاش', 'ساخ', 'به', 'پا', 'رسیدددددددد']\n",
            "**********************\n",
            "اللة اکبر گویان وارد مسجد شد\n",
            "['اللة', 'اکبر', 'گو', 'وارد', 'مسجد', 'شد']\n",
            "**********************\n",
            "دکتر جان مممممممممممممممنونم که مشکلات حرکتي فرزندم را درمان کردید\n",
            "['دک', 'ج', 'مممممممممممممممنون', 'که', 'مشکل', 'حرکت', 'فرزند', 'را', 'در', 'کردید']\n",
            "**********************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMXOCcQ15PQM"
      },
      "source": [
        "# **Parsivar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlpQxAUU5T-x",
        "outputId": "7692c49e-1297-4891-ae3b-92d365db8e2b"
      },
      "source": [
        "from parsivar import Tokenizer\n",
        "from parsivar import Normalizer\n",
        "from parsivar import FindStems\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "normalizer = Normalizer()\n",
        "stemmer = FindStems()\n",
        "\n",
        "for sent in sentences:\n",
        "  stems = []\n",
        "  print(sent)\n",
        "  words = tokenizer.tokenize_words(normalizer.normalize(sent))\n",
        "  for word in words:\n",
        "    stems.append(stemmer.convert_to_stem(word))\n",
        "  print(stems)\n",
        "  print(\"**********************\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "انتخابات هفته آینده رادر تهران برگزار میکنیم ، اما دسته بندی حوزهها برای رأی گیری از این هفته آغاز شده است\n",
            "['انتخابات', 'هفته', 'آینده', 'رادر', 'تهران', 'برگزار', 'کرد&کن', '،', 'اما', 'دسته\\u200cبندی', 'حوزه', 'برای', 'رای\\u200cگیری', 'از', 'این', 'هفته', 'آغاز\\u200cشده\\u200cاست']\n",
            "**********************\n",
            "توانتشارخبرمهم در رسانه ها را تایید کردی\n",
            "['توانتشارخبرمهم', 'در', 'رسانه', 'را', 'تایید', 'کردی']\n",
            "**********************\n",
            "پسردکتراحمدی درکارنامه حرفه ای خود ۱۸سریال تلویزیونی و ۱۲فیلم سینمایی دارد\n",
            "['پسردکتراحمدی', 'درکارنامه', 'حرفه\\u200cای', 'خود', '18سریال', 'تلویزیونی', 'و', '12فیلم', 'سینمایی', 'داشت&دارد']\n",
            "**********************\n",
            "نقّاشی ساختمان به پایان رسیدددددددد\n",
            "['نقّاشی', 'ساختمان', 'به', 'پایان', 'رسیدددددددد']\n",
            "**********************\n",
            "اللة اکبر گویان وارد مسجد شد\n",
            "['الله', 'اکبر', 'گوی', 'وارد', 'مسجد', 'شد']\n",
            "**********************\n",
            "دکتر جان مممممممممممممممنونم که مشکلات حرکتي فرزندم را درمان کردید\n",
            "['دکتر', 'جان', 'مممممممممممممممنونم', 'که', 'مشکلات', 'حرکتی', 'فرزند', 'را', 'درمان', 'کرد&کن']\n",
            "**********************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJwph-kJ5Uo0"
      },
      "source": [
        "# **Lemmatization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmSXIRfi5aA8"
      },
      "source": [
        "# **Hazm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKMtEkab5kd9",
        "outputId": "c92cc33d-802f-45db-c6af-2e392d078048"
      },
      "source": [
        "from hazm import Lemmatizer\n",
        "from hazm import Normalizer\n",
        "\n",
        "normalizer = Normalizer()\n",
        "lemmatizer = Lemmatizer()\n",
        "\n",
        "for sent in sentences:\n",
        "  lems = []\n",
        "  print(sent)\n",
        "  words = hazm.word_tokenize(normalizer.normalize(sent))\n",
        "  for word in words:\n",
        "    lems.append(lemmatizer.lemmatize(word))\n",
        "  print(lems)\n",
        "  print(\"**********************\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "انتخابات هفته آینده رادر تهران برگزار میکنیم ، اما دسته بندی حوزهها برای رأی گیری از این هفته آغاز شده است\n",
            "['انتخابات', 'هفته', 'آینده', 'رادر', 'تهران', 'برگزار', 'میکنیم', '،', 'اما', 'دسته', 'بست#بند', 'حوزه', 'برای', 'رأی', 'گرفت#گیر', 'از', 'این', 'هفته', 'آغاز', 'شد#شو']\n",
            "**********************\n",
            "توانتشارخبرمهم در رسانه ها را تایید کردی\n",
            "['توانتشارخبرمهم', 'در', 'رسانه', 'را', 'تایید', 'کردی']\n",
            "**********************\n",
            "پسردکتراحمدی درکارنامه حرفه ای خود ۱۸سریال تلویزیونی و ۱۲فیلم سینمایی دارد\n",
            "['پسردکتراحمدی', 'درکارنامه', 'حرفه\\u200cای', 'خود', '۱۸', 'سریال', 'تلویزیون', 'و', '۱۲', 'فیلم', 'سینمایی', 'داشت#دار']\n",
            "**********************\n",
            "نقّاشی ساختمان به پایان رسیدددددددد\n",
            "['نقاش', 'ساختمان', 'به', 'پایان', 'رسیدددددددد']\n",
            "**********************\n",
            "اللة اکبر گویان وارد مسجد شد\n",
            "['اللة', 'اکبر', 'گویان', 'وارد', 'مسجد', 'شد#شو']\n",
            "**********************\n",
            "دکتر جان مممممممممممممممنونم که مشکلات حرکتي فرزندم را درمان کردید\n",
            "['دکتر', 'جان', 'مممممممممممممممنونم', 'که', 'مشکلات', 'حرکت', 'فرزند', 'را', 'درمان', 'کرد#کن']\n",
            "**********************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTgss33_5lCK"
      },
      "source": [
        "# **Parsivar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTz_YgBl5orH"
      },
      "source": [
        "# Not Exist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyS1AoUY5si8"
      },
      "source": [
        "# **Stopword Removal**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fJfoVOa5uXA"
      },
      "source": [
        "# **Hazm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et-l2dTo5xRW",
        "outputId": "66492d9e-bff2-4c81-b4e6-95ea428df7be"
      },
      "source": [
        "from hazm import stopwords_list\n",
        "from hazm import Normalizer\n",
        "\n",
        "stopwords = stopwords_list(\"/content/hazm/hazm/data/stopwords.dat\")\n",
        "\n",
        "for sent in sentences:\n",
        "  sents_without_stopword = []\n",
        "  print(sent)\n",
        "  words = hazm.word_tokenize(normalizer.normalize(sent))\n",
        "  for word in words:\n",
        "    if word not in stopwords:\n",
        "      sents_without_stopword.append(word)\n",
        "  print(sents_without_stopword)\n",
        "  print(\"**********************\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "انتخابات هفته آینده رادر تهران برگزار میکنیم ، اما دسته بندی حوزهها برای رأی گیری از این هفته آغاز شده است\n",
            "['انتخابات', 'هفته', 'آینده', 'رادر', 'تهران', 'برگزار', 'میکنیم', '،', 'حوزهها', 'رأی', 'هفته', 'آغاز']\n",
            "**********************\n",
            "توانتشارخبرمهم در رسانه ها را تایید کردی\n",
            "['توانتشارخبرمهم', 'رسانه\\u200cها', 'تایید', 'کردی']\n",
            "**********************\n",
            "پسردکتراحمدی درکارنامه حرفه ای خود ۱۸سریال تلویزیونی و ۱۲فیلم سینمایی دارد\n",
            "['پسردکتراحمدی', 'درکارنامه', 'حرفه\\u200cای', '۱۸', 'سریال', 'تلویزیونی', '۱۲', 'فیلم', 'سینمایی']\n",
            "**********************\n",
            "نقّاشی ساختمان به پایان رسیدددددددد\n",
            "['نقاشی', 'ساختمان', 'پایان', 'رسیدددددددد']\n",
            "**********************\n",
            "اللة اکبر گویان وارد مسجد شد\n",
            "['اللة', 'اکبر', 'گویان', 'مسجد']\n",
            "**********************\n",
            "دکتر جان مممممممممممممممنونم که مشکلات حرکتي فرزندم را درمان کردید\n",
            "['دکتر', 'جان', 'مممممممممممممممنونم', 'مشکلات', 'حرکتی', 'فرزندم', 'درمان', 'کردید']\n",
            "**********************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US94nnvE5xp9"
      },
      "source": [
        "# **Parsivar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVGsfQxH501Z"
      },
      "source": [
        "# Not Exist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6KMj_Z29UZt"
      },
      "source": [
        "# **Q5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d997ntfMRcR"
      },
      "source": [
        "from operator import itemgetter\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clvrg2fn9kMQ"
      },
      "source": [
        "# read wrong spelled words and their phrase\n",
        "with open(\"Q6-Sentences.txt\") as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "phrases = []\n",
        "wrong_spelled_words = []\n",
        "\n",
        "for line in lines:\n",
        "  wrong_spelled_words.append(line.split(\":\")[0].strip())\n",
        "  phrases.append(line.split(\":\")[1].strip())\n",
        "\n",
        "# read third homework data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "data = pd.concat([train, test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pFfRKVg96FI"
      },
      "source": [
        "# create dictionary of words\n",
        "def create_words_dic(data):\n",
        "\n",
        "  words_dic = {}\n",
        "\n",
        "  for index, row in data.iterrows():\n",
        "    words = row.article.split(\" \")\n",
        "    for word in words:\n",
        "      if word not in words_dic.keys():\n",
        "        words_dic[word] = 1\n",
        "      else:\n",
        "        words_dic[word] += 1\n",
        "        \n",
        "  return words_dic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzpUa_r4_aqC"
      },
      "source": [
        "words_dic = create_words_dic(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyeBcybiY_tc"
      },
      "source": [
        "# remove rare words from dictionary\n",
        "words_dic = {k:v for (k, v) in words_dic.items() if v > 3}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X06b9VNLZPro",
        "outputId": "fab950d8-df05-43bb-c9b7-c767677cf424"
      },
      "source": [
        "len(words_dic.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25578"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBzk4iJpHym7"
      },
      "source": [
        "def levenshtein(word1, word2):\n",
        "\n",
        "    size_x = len(word1) + 1\n",
        "    size_y = len(word2) + 1\n",
        "\n",
        "    matrix = np.zeros ((size_x, size_y))\n",
        "\n",
        "    for x in range(size_x):\n",
        "        matrix[x, 0] = x\n",
        "\n",
        "    for y in range(size_y):\n",
        "        matrix[0, y] = y\n",
        "\n",
        "    for x in range(1, size_x):\n",
        "        for y in range(1, size_y):\n",
        "            if word1[x-1] == word2[y-1]:\n",
        "                matrix[x,y] = min(matrix[x-1, y] + 1, matrix[x-1, y-1], matrix[x, y-1] + 1)\n",
        "            else:\n",
        "                matrix[x,y] = min(matrix[x-1, y] + 1, matrix[x-1, y-1] + 2, matrix[x, y-1] + 1)\n",
        "\n",
        "    return matrix[size_x-1, size_y-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67dL3SjUI2u6",
        "outputId": "d948aa8f-b2fa-4918-d673-a491f6b328a3"
      },
      "source": [
        "levenshtein(\"fast\", \"cats\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.0"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKdbiJ6QLiBG"
      },
      "source": [
        "def find_bottom5_levenshtein(words_dic, target_word):\n",
        "\n",
        "  levenshtein_dis =[]\n",
        "\n",
        "  for key, value in words_dic.items():\n",
        "    levenshtein_dis.append([key, levenshtein(key, target_word)])\n",
        "  \n",
        "  sorted_lev_dis = sorted(levenshtein_dis, key=itemgetter(1))\n",
        "\n",
        "  return sorted_lev_dis[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKiIXp2YcQKp",
        "outputId": "e443c5ef-1515-423a-cc4b-64ccb6aec91b"
      },
      "source": [
        "for word in wrong_spelled_words:\n",
        "  print(word)\n",
        "  print(find_bottom5_levenshtein(words_dic, word))\n",
        "  print(\"*************************\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "اختصاد\n",
            "[['اختصاص', 2.0], ['اقتصاد', 2.0], ['اختصار', 2.0], ['اقتصادي', 3.0], ['افتاد', 3.0]]\n",
            "*************************\n",
            "سادرات\n",
            "[['سادات', 1.0], ['خسارات', 2.0], ['صادرات', 2.0], ['سارا', 2.0], ['ادات', 2.0]]\n",
            "*************************\n",
            "فوتكال\n",
            "[['فوتبال', 2.0], ['فوتسال', 2.0], ['فوت', 3.0], ['فواصل', 3.0], ['تكامل', 3.0]]\n",
            "*************************\n",
            "مسابغاط\n",
            "[['سابا', 3.0], ['مساال', 4.0], ['مسابقات', 4.0], ['محابا', 4.0], ['سابقا', 4.0]]\n",
            "*************************\n",
            "وازدات\n",
            "[['واداشت', 2.0], ['موازات', 2.0], ['واردات', 2.0], ['ادات', 2.0], ['وزارت', 3.0]]\n",
            "*************************\n",
            "مشركت\n",
            "[['شركت', 1.0], ['مشاركت', 1.0], ['مشرك', 1.0], ['مشترك', 2.0], ['شركتي', 2.0]]\n",
            "*************************\n",
            "کشوور\n",
            "[['شور', 2.0], ['شوكور', 2.0], ['شعور', 3.0], ['كشور', 3.0], ['شوهر', 3.0]]\n",
            "*************************\n",
            "منجلسه\n",
            "[['مجلس', 2.0], ['جلسه', 2.0], ['مجله', 2.0], ['منجمله', 2.0], ['مجسمه', 3.0]]\n",
            "*************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11q2EjG2mmKl"
      },
      "source": [
        "# **Q6**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHPyM2Get8Um"
      },
      "source": [
        "# calculate unigram probabilities for each word (w_i) and store it\n",
        "\n",
        "def calc_unigram_probs(corpus):\n",
        "  \n",
        "  word_count = {}\n",
        "  total_words = 0\n",
        "\n",
        "  for doc in corpus:\n",
        "    tmp = doc.split(' ')\n",
        "    total_words += len(tmp)\n",
        "\n",
        "    for word in tmp:\n",
        "      if (not (word in word_count)):\n",
        "        word_count[word] = 0\n",
        "      word_count[word] = word_count[word] + 1\n",
        "\n",
        "  return word_count, total_words\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2I_OGkgt9nV"
      },
      "source": [
        "word_count, total_words = calc_unigram_probs(train.article.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7TRw4b4pR9F"
      },
      "source": [
        "# Bigrams are extracted from input corpus\n",
        "\n",
        "def collect_bigrams(corpus):\n",
        "  \n",
        "  word_bigrams = {}\n",
        "  \n",
        "  for doc in corpus:\n",
        "    tmp = doc.split(' ')\n",
        "\n",
        "    for i in range(1, len(tmp)):\n",
        "      word = tmp[i]\n",
        "      prev_word = tmp[i-1]\n",
        "      \n",
        "      if not (word in word_bigrams):\n",
        "        word_bigrams[word] = []\n",
        "\n",
        "      word_bigrams[word].append(prev_word)\n",
        "\n",
        "  return word_bigrams\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsytHeCIpZsQ"
      },
      "source": [
        "# The bigram LM probability of the given bigram is calculated\n",
        "\n",
        "def get_bigram_prob(word, prev_word):\n",
        "  \n",
        "  bigrams = word_bigrams[word]\n",
        "  count = 0\n",
        "  \n",
        "  for elem in bigrams:\n",
        "    if elem == prev_word:\n",
        "      count += 1\n",
        "\n",
        "  N = word_count[prev_word]\n",
        "  prob = count / N\n",
        "\n",
        "  return prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6UcWthmpk5J"
      },
      "source": [
        "# The bigram LM probability of the input sentence is calculated\n",
        "\n",
        "def bigram_LM(sent):\n",
        "  \n",
        "  prob = 1\n",
        "  tmp = sent.split(' ')\n",
        "  prob = prob * (word_count[tmp[0]]/total_words)\n",
        "\n",
        "  for i in range(1, len(tmp)):\n",
        "    prob = prob * get_bigram_prob(tmp[i], tmp[i-1])\n",
        "\n",
        "  return prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkFQnvHFpWpc"
      },
      "source": [
        "word_bigrams = collect_bigrams(train.article.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifb9_8Cspn7d"
      },
      "source": [
        "def get_most_possible_correct_spell(levenshtein_result, phrase, target_word):\n",
        "  probs = []\n",
        "  for res in levenshtein_result:\n",
        "    corrected = phrase.replace(target_word, res[0]);\n",
        "    probs.append([res[0], bigram_LM(corrected)])\n",
        "  # print(probs)\n",
        "  return sorted(probs, key=itemgetter(1), reverse=True)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyYh5MkdOuMg",
        "outputId": "21ca8ece-4a16-42c8-c1f1-5d0320639190"
      },
      "source": [
        "for i in range(len(phrases)):\n",
        "\n",
        "  print(\"Phrase:\")\n",
        "  print(phrases[i])\n",
        "  print()\n",
        "  levenshtein_result = find_bottom5_levenshtein(words_dic, wrong_spelled_words[i])\n",
        "  print(\"Levenshtein result:\")\n",
        "  print(levenshtein_result)\n",
        "  print()\n",
        "  print(\"Most possible correct spell:\")\n",
        "  print(get_most_possible_correct_spell(levenshtein_result, phrases[i], wrong_spelled_words[i]))\n",
        "  print()\n",
        "  print(\"**************************\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phrase:\n",
            "رشد اختصاد و تحرك زندگي اجتماعي\n",
            "\n",
            "Levenshtein result:\n",
            "[['اختصاص', 2.0], ['اقتصاد', 2.0], ['اختصار', 2.0], ['اقتصادي', 3.0], ['افتاد', 3.0]]\n",
            "\n",
            "Most possible correct spell:\n",
            "['اقتصادي', 7.192338206471085e-14]\n",
            "\n",
            "**************************\n",
            "Phrase:\n",
            "حجم سادرات ايران\n",
            "\n",
            "Levenshtein result:\n",
            "[['سادات', 1.0], ['خسارات', 2.0], ['صادرات', 2.0], ['سارا', 2.0], ['ادات', 2.0]]\n",
            "\n",
            "Most possible correct spell:\n",
            "['صادرات', 6.186026638465862e-08]\n",
            "\n",
            "**************************\n",
            "Phrase:\n",
            "فدراسيون فوتكال كشور\n",
            "\n",
            "Levenshtein result:\n",
            "[['فوتبال', 2.0], ['فوتسال', 2.0], ['فوت', 3.0], ['فواصل', 3.0], ['تكامل', 3.0]]\n",
            "\n",
            "Most possible correct spell:\n",
            "['فوتبال', 1.8976452347823305e-07]\n",
            "\n",
            "**************************\n",
            "Phrase:\n",
            "در جريان انعكاس مسابغاط صبح\n",
            "\n",
            "Levenshtein result:\n",
            "[['سابا', 3.0], ['مساال', 4.0], ['مسابقات', 4.0], ['محابا', 4.0], ['سابقا', 4.0]]\n",
            "\n",
            "Most possible correct spell:\n",
            "['مسابقات', 1.1269907352868415e-11]\n",
            "\n",
            "**************************\n",
            "Phrase:\n",
            "اقلام عمده وازدات كشور\n",
            "\n",
            "Levenshtein result:\n",
            "[['واداشت', 2.0], ['موازات', 2.0], ['واردات', 2.0], ['ادات', 2.0], ['وزارت', 3.0]]\n",
            "\n",
            "Most possible correct spell:\n",
            "['واردات', 1.5307165015302917e-10]\n",
            "\n",
            "**************************\n",
            "Phrase:\n",
            "اصل مشركت مردمي\n",
            "\n",
            "Levenshtein result:\n",
            "[['شركت', 1.0], ['مشاركت', 1.0], ['مشرك', 1.0], ['مشترك', 2.0], ['شركتي', 2.0]]\n",
            "\n",
            "Most possible correct spell:\n",
            "['مشاركت', 3.494107304501202e-08]\n",
            "\n",
            "**************************\n",
            "Phrase:\n",
            "وزارت کشوور جمهوري اسلامي ايران\n",
            "\n",
            "Levenshtein result:\n",
            "[['شور', 2.0], ['شوكور', 2.0], ['شعور', 3.0], ['كشور', 3.0], ['شوهر', 3.0]]\n",
            "\n",
            "Most possible correct spell:\n",
            "['كشور', 5.402225163735894e-09]\n",
            "\n",
            "**************************\n",
            "Phrase:\n",
            "جلسه علني ديروز منجلسه شوراي اسلامي\n",
            "\n",
            "Levenshtein result:\n",
            "[['مجلس', 2.0], ['جلسه', 2.0], ['مجله', 2.0], ['منجمله', 2.0], ['مجسمه', 3.0]]\n",
            "\n",
            "Most possible correct spell:\n",
            "['مجلس', 5.269239684481391e-09]\n",
            "\n",
            "**************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54vU-yExuPPe"
      },
      "source": [
        "# **Extra (Substitution Cost == 1)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljNQRkHtuO0G"
      },
      "source": [
        "def levenshtein(word1, word2):\n",
        "\n",
        "    size_x = len(word1) + 1\n",
        "    size_y = len(word2) + 1\n",
        "\n",
        "    matrix = np.zeros ((size_x, size_y))\n",
        "\n",
        "    for x in range(size_x):\n",
        "        matrix[x, 0] = x\n",
        "\n",
        "    for y in range(size_y):\n",
        "        matrix[0, y] = y\n",
        "\n",
        "    for x in range(1, size_x):\n",
        "        for y in range(1, size_y):\n",
        "            if word1[x-1] == word2[y-1]:\n",
        "                matrix[x,y] = min(matrix[x-1, y] + 1, matrix[x-1, y-1], matrix[x, y-1] + 1)\n",
        "            else:\n",
        "                matrix[x,y] = min(matrix[x-1, y] + 1, matrix[x-1, y-1] + 1, matrix[x, y-1] + 1)\n",
        "\n",
        "    return matrix[size_x-1, size_y-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TU6RYl0Vgouh",
        "outputId": "dc7be3d6-e1d3-4088-b044-0afeb22fdc48"
      },
      "source": [
        "for i in range(len(phrases)):\n",
        "\n",
        "  print(\"Phrase:\")\n",
        "  print(phrases[i])\n",
        "  print()\n",
        "  levenshtein_result = find_bottom5_levenshtein(words_dic, wrong_spelled_words[i])\n",
        "  print(\"Levenshtein result:\")\n",
        "  print(levenshtein_result)\n",
        "  print()\n",
        "  print(\"Most possible correct spell:\")\n",
        "  print(get_most_possible_correct_spell(levenshtein_result, phrases[i], wrong_spelled_words[i]))\n",
        "  print()\n",
        "  print(\"**************************\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phrase:\n",
            "رشد اختصاد و تحرك زندگي اجتماعي\n",
            "\n",
            "Levenshtein result:\n",
            "[['اختصاص', 1.0], ['اقتصاد', 1.0], ['اختصار', 1.0], ['اعتقاد', 2.0], ['اعتماد', 2.0]]\n",
            "\n",
            "Most possible correct spell:\n",
            "['اقتصاد', 2.8596068096254927e-15]\n",
            "\n",
            "**************************\n",
            "Phrase:\n",
            "حجم سادرات ايران\n",
            "\n",
            "Levenshtein result:\n",
            "[['صادرات', 1.0], ['سادات', 1.0], ['ادراك', 2.0], ['ماديات', 2.0], ['عادات', 2.0]]\n",
            "\n",
            "Most possible correct spell:\n",
            "['صادرات', 6.186026638465862e-08]\n",
            "\n",
            "**************************\n",
            "Phrase:\n",
            "فدراسيون فوتكال كشور\n",
            "\n",
            "Levenshtein result:\n",
            "[['فوتبال', 1.0], ['فوتسال', 1.0], ['فوتبالي', 2.0], ['نوترال', 2.0], ['فوتبال،', 2.0]]\n",
            "\n",
            "Most possible correct spell:\n",
            "['فوتبال', 1.8976452347823305e-07]\n",
            "\n",
            "**************************\n",
            "Phrase:\n",
            "در جريان انعكاس مسابغاط صبح\n",
            "\n",
            "Levenshtein result:\n",
            "[['مسابقات', 2.0], ['مساال', 3.0], ['مساولان', 3.0], ['مسابقه', 3.0], ['مساوات', 3.0]]\n",
            "\n",
            "Most possible correct spell:\n",
            "['مسابقات', 1.1269907352868415e-11]\n",
            "\n",
            "**************************\n",
            "Phrase:\n",
            "اقلام عمده وازدات كشور\n",
            "\n",
            "Levenshtein result:\n",
            "[['واردات', 1.0], ['وادار', 2.0], ['وارداتي', 2.0], ['بازداشت', 2.0], ['عادات', 2.0]]\n",
            "\n",
            "Most possible correct spell:\n",
            "['واردات', 1.5307165015302917e-10]\n",
            "\n",
            "**************************\n",
            "Phrase:\n",
            "اصل مشركت مردمي\n",
            "\n",
            "Levenshtein result:\n",
            "[['شركت', 1.0], ['مشاركت', 1.0], ['مشرك', 1.0], ['مشكل', 2.0], ['حركت', 2.0]]\n",
            "\n",
            "Most possible correct spell:\n",
            "['مشاركت', 3.494107304501202e-08]\n",
            "\n",
            "**************************\n",
            "Phrase:\n",
            "وزارت کشوور جمهوري اسلامي ايران\n",
            "\n",
            "Levenshtein result:\n",
            "[['شور', 2.0], ['شعور', 2.0], ['كشور', 2.0], ['دشوار', 2.0], ['مشاور', 2.0]]\n",
            "\n",
            "Most possible correct spell:\n",
            "['كشور', 5.402225163735894e-09]\n",
            "\n",
            "**************************\n",
            "Phrase:\n",
            "جلسه علني ديروز منجلسه شوراي اسلامي\n",
            "\n",
            "Levenshtein result:\n",
            "[['مجلس', 2.0], ['جلسه', 2.0], ['مجلس،', 2.0], ['منزله', 2.0], ['مجله', 2.0]]\n",
            "\n",
            "Most possible correct spell:\n",
            "['مجلس', 5.269239684481391e-09]\n",
            "\n",
            "**************************\n"
          ]
        }
      ]
    }
  ]
}