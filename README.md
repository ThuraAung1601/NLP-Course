# Natural Language Processing

## Project

Implemented a ***chatbot*** which using <u>intent detection model</u> undrestand user intention and using <u>slotfilling model</u> get information from user query and will recommend us a resturant.

## Course material

### week 1

- **Class:** Lingusitics knowledge, NLP challenges, Probabilistic Language modeling, Word token vs. Word type
- **Homework:** Unigram Language model, Bigram Language model, N-gram Language model, Zipfs law, Words probability using Ngram models.

### week 2

- **Class:** Smoothing, Laplace smoothing, backoff and interpolation, Bayesian smoothing with drichlet prior, Absolute discounting, Kneser-ney Smoothing, Bayesian smoothing based on  pitman-yor process, Entropy, Perplexity

- **Homework:** Bayesian smoothing with dirichlet prior, Perplexity of sentence and corpus, trigram neural language model using feed forward network
  
  Check this article:
  
  1. [Neural language model](https://towardsdatascience.com/neural-language-models-32bec14d01dc)

### week 3

- **Class:** Spare word representation, Dense word representation, term-document matrix, cosine similarity, tf-idf, positive point wise mutual information, SVD, brow clustering, skip-gram and CBOW, GLOVE

- **Homework:** gensim word2vec and doc2vec, TSNE for visualizing high dimensional plots, TF-IDF implementation, find document similarity using word2vec weighted mean average by tf-idf and doc2vec model
  
  Check this articke:
  
  1. [Word Embedding](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)
  2. [Introduction to Word Embedding and Word2Vec](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)
  3. [Difference between Word2vec and onehot representation](https://datascience.stackexchange.com/questions/29851/one-hot-encoding-vs-word-embedding-when-to-choose-one-or-another#:~:text=They%20also%20differ%20at%20the,together%20in%20the%20representation%20space.)

### week 4

- **Class:** Tokenization, Normalization, Lemmatization, Stemming, Stopword removal, Minimum edit distance, 

- **Homework:** Hazm and parsivar Library, Levenshtein distance calculation

### week 5

- **Class:** Recurrent neural networks, Vanishing graident descent, LSTM, GRU, Bidirectional RNN, Bidirectional LSTM, Bidirectional GRU, Transformers, Attention.

- **Homework:** Word2Vec model, RNN, LSTM, GRU, Attention, f1-score, Accuracy, Recall, Precision.
  
  Check these articles:
  
  1. [RNN, LSTM, GRU ](https://medium.com/analytics-vidhya/rnn-vs-gru-vs-lstm-863b0b7b1573)
  
  2. [Encoder Decoder model](https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346)
  
  3. [Attention, Self attention (transformers)](https://medium.com/towards-data-science/what-is-attention-mechanism-can-i-have-your-attention-please-3333637f2eac)
  
  4. [Evaluation metrics](https://towardsdatascience.com/a-look-at-precision-recall-and-f1-score-36b5fd0dd3ec)
  
  5. [Difference between word2vec and embedding layer](https://datascience.stackexchange.com/questions/61603/difference-between-gensim-word2vec-and-keras-embedding-layer)

### week 6

- **Class:** Dialogue systems, general chatbots (Conversational agent), tasked-based chatbots, rule base or corpus base conversational agents, Turning, Speech act, grounding, subdialogues, initative, inference, Eliza and Parry as Rule base conversational agent, information retrival in corpus base, Neural text matching, Representation base model, interaction base model, hybrid model, Generation methods,

- **Homework:** Seq2Seq models, Dialogue systems, Transformers, Encoder-decoders.
  
  Check these articles:
  
  1. [Transformers](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452)
  
  2. [Transformer architecture](https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34)
  
  3. [Attention architecture](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)
  
  4. [Why attentions](https://towardsdatascience.com/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3)

### week 7

- **Class:** Contextualized representation, Elmo, Bert, masked language model, next sentence prediction, Bert family, GPT,

- **Homework:** Transformer, Bert Tokenizer, Retrival based chatbot
  
  Check these articles:
  
  1. [Statistic vs Contextualizer word representation](https://ted-mei.medium.com/from-static-embedding-to-contextualized-embedding-fe604886b2bc#:~:text=The%20main%20problem%20that%20word,considers%20contextual%20information%20into%20play.)
  
  2. [GPT models](**[The journey of open ai gpt models](https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2)**)
  
  3. [BERT](**[Deep explanation fo Bert](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)**)
  
  4. [BERT family](https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8)

## Project
